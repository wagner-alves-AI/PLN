{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classificador_de_SPAM.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNw047iGr18zX8ff6QQZnI7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wagner-alves-AI/PLN/blob/master/Classificador_de_SPAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOuwAEdTg0hU",
        "colab_type": "text"
      },
      "source": [
        "ETAPA DE OBTENÇÃO DOS DADOS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It-N-9yUXucC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests #biblioteca HTTP\n",
        "import zipfile  #biblioteca de compressão de arquivos\n",
        "import io # biblioteca para lidar com entrada e saída\n",
        "\n",
        "#URL  do arquivo TSV\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
        "\n",
        "# Download do arquivos TSV\n",
        "r = requests.get(url)\n",
        "\n",
        "# Instanciando o compressor ZipFile\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "\n",
        "# Extração (descompressão) dos dados\n",
        "z.extractall()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwQBk8ZhZ_2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Carregar o dataset de mensagens para um Dataframe\n",
        "df = pd.read_table('/content/SMSSpamCollection', header=None, encoding='utf-8')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nTIlWZRb-cj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "2c3787a1-2463-456d-cd46-726c1085a334"
      },
      "source": [
        "print(df.info())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5572 entries, 0 to 5571\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   0       5572 non-null   object\n",
            " 1   1       5572 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 87.2+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEq8A7vbcW_V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "27324104-e02f-4b20-d910-fdc74886a426"
      },
      "source": [
        "print(df.head())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      0                                                  1\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Npsh_vA8ciFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Variável para as classes para evitar ficar usando a notação df[0]\n",
        "classes = df[0]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKUJlAr9c3ul",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5b89ccdf-7469-441b-cf8b-ed787700e956"
      },
      "source": [
        "# imprimir a distribuição das classes\n",
        "print(classes.value_counts())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ham     4825\n",
            "spam     747\n",
            "Name: 0, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6T_oKw4guHF",
        "colab_type": "text"
      },
      "source": [
        "ETAPA DE PRÉ-PROCESSAMENTO COM REGEX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtpO0X0UdQgt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "ad80b3b3-6e92-4a59-8db3-7c99de76f7a9"
      },
      "source": [
        "import re\n",
        "\n",
        "# padrão de re para encontrar números e caracteres especiais\n",
        "pattern = re.compile(r'[\\d@_!#$%^&*()<>?/\\|}{~:]')\n",
        "\n",
        "# selecionar a coluna de mensagens\n",
        "text_messages = df[1]\n",
        "\n",
        "# selecionar as 10 primeiras linhas\n",
        "messages = text_messages[0:10]\n",
        "\n",
        "# para cada linha na lista de mensagens\n",
        "for line in messages:\n",
        "  # criar lista de palavras dividindo a frase pelos espaços em branco\n",
        "  words = line.split(' ')\n",
        "  # para cada palavra na lista de palavras:\n",
        "  for word in words:\n",
        "    if re.search(pattern, word):\n",
        "      print(word)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "21st\n",
            "2005.\n",
            "87121\n",
            "question(std\n",
            "rate)T&C's\n",
            "08452810075over18's\n",
            "3\n",
            "back!\n",
            "still?\n",
            "ok!\n",
            "£1.50\n",
            "(Oru\n",
            "Vettam)'\n",
            "*9\n",
            "WINNER!!\n",
            "£900\n",
            "reward!\n",
            "09061701461.\n",
            "KL341.\n",
            "12\n",
            "11\n",
            "more?\n",
            "Free!\n",
            "08002986030\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdLoVvg_fy-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# alterar todas as palavras para minúsculas\n",
        "processed_lines = text_messages.str.lower()\n",
        "\n",
        "# Lista de tuplas com os padrões e novas palavras\n",
        "patterns = [\n",
        "            # Substituir endereços de e-mail por 'emailaddress'\n",
        "            (r'^.+@[^\\.].*\\.[a-z]{2,}$', 'emailaddress'),\n",
        "            # Substituir URLS´s por 'webaddress'\n",
        "            (r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$','webaddress'),\n",
        "            # Substituir simbolos monetários por 'moneysymb'\n",
        "            (r'£|\\$', 'moneysymb'),\n",
        "            # Substituir números por 'numbr'\n",
        "            (r'\\d+(\\.\\d+)?', 'numbr'),\n",
        "            # Remover pontuação\n",
        "            (r'[^\\w\\d\\s]', ''),\n",
        "            # Substitua dois ou mais espaços em branco por um único espaço\n",
        "            (r'\\s+', ' '),\n",
        "            # Remova os espaços em branco à esquerda e à direita\n",
        "            (r'^\\s+|\\s+?$', '')\n",
        "]\n",
        "\n",
        "for pattern, newword in patterns:\n",
        "  processed_lines = processed_lines.str.replace(pattern, newword)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES53T2NxztTR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "049044af-f0d7-4786-cb63-75c853405ec5"
      },
      "source": [
        "print(processed_lines)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       go until jurong point crazy available only in ...\n",
            "1                                 ok lar joking wif u oni\n",
            "2       free entry in numbr a wkly comp to win fa cup ...\n",
            "3             u dun say so early hor u c already then say\n",
            "4       nah i dont think he goes to usf he lives aroun...\n",
            "                              ...                        \n",
            "5567    this is the numbrnd time we have tried numbr c...\n",
            "5568                  will ü b going to esplanade fr home\n",
            "5569    pity was in mood for that soany other suggestions\n",
            "5570    the guy did some bitching but i acted like id ...\n",
            "5571                            rofl its true to its name\n",
            "Name: 1, Length: 5572, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkPywCwi1XoR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5a276305-5ff6-45f6-f8f5-3b66b5cb0ac4"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# criar um lista das stop words\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# inicializar uma instância de Porter Stemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Para cada índice na lista\n",
        "for i in range(len(processed_lines)):\n",
        "  # criar tokens para cada linhas\n",
        "  word_tokens = word_tokenize(processed_lines[i])\n",
        "  # inicializar uma lista para receber as palavras filtradas\n",
        "  filtered_sentence = []\n",
        "  #para cada palavra da lista de tokens\n",
        "  for word in word_tokens:\n",
        "    if word not in stop_words:\n",
        "      # cria um stem para a palavra\n",
        "      stemmed_word = ps.stem(word)\n",
        "      # adiciona na lista de palavras filtradas\n",
        "      filtered_sentence.append(stemmed_word)\n",
        "\n",
        "      # Junta as palavras da lista para substituir o antigo \n",
        "      # texto pelo novo texto processado:\n",
        "      processed_lines[i] = ''.join(filtered_sentence)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcH0QH-p4YUv",
        "colab_type": "text"
      },
      "source": [
        "**Construção** **do** **modelo** **de** **predição**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zadlWYBe4c9f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "b366d70f-c030-4c5e-8b99-372dd3d6ead7"
      },
      "source": [
        "#inicializar um lista de palavras:\n",
        "all_words = []\n",
        "\n",
        "# para cada linha da lista de linhas que foram processadas:\n",
        "for line in processed_lines:\n",
        "  #criar lista de palavras tokenizadas para cada linha:\n",
        "  word_tokens = word_tokenize(line)\n",
        "  # para cada palavra da lista de palavras:\n",
        "  for word in word_tokens:\n",
        "  # adiciona o token na bag onde estão todos os tokens\n",
        "    all_words.append(word)\n",
        "\n",
        "# Atribui a contagem de de frequência para cada token da lista:\n",
        "all_words = nltk.FreqDist(all_words)\n",
        "\n",
        "print(f'Total de palavras: {len(all_words)}')\n",
        "print(f'Palavras mais comuns: {all_words.most_common(10)}')\n",
        "\n",
        "#Usar as 1500 palavras mais comuns como modelo de features\n",
        "# O método keys() da classe 'FreqDist' retorna somente\n",
        "# os termos, que são as chaves da estrutura 'dict _ keys',\n",
        "# por isso, foi usado list() para converter 'dict _ keys'\n",
        "# para o tipo list\n",
        "word_features = list(all_words.keys())[0:1500]\n",
        "# Imprimir as 10 primeiras features (opcional)\n",
        "print(f'Lista de características: {word_features[0:10]}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total de palavras: 5088\n",
            "Palavras mais comuns: [('sorriillcalllater', 30), ('ok', 20), ('cantpickphonerightplsendmessag', 12), ('oki', 7), ('oklor', 5), ('sorriillcalllatermeet', 4), ('pleascallcustomservicrepresfreephonnumbrnumbrnumbrnumbramnumbrpmguarantemoneysymbnumbrcashmoneysymbnumbrprize', 4), ('wenurlovablbcumangriwidudnttakeseriouscozangrichildishntruewayshowdeepaffectcarenluvkettodamandanicedayda', 4), ('opinionnumbrnumbrjadanumbrkusruthinumbrlovablnumbrsilentnumbrsplcharactnumbrmaturnumbrstylishnumbrsimplplrepli', 4), ('privatnumbraccountstatementshownumbrunredeempointcallnumbridentificodenumbrexpirnumbrnumbrnumbr', 4)]\n",
            "Lista de características: ['gojurongpointcraziavailbugingreatworldlaebuffetcinegotamorwat', 'oklarjokewifuoni', 'freeentrinumbrwklicompwinfacupfinaltktnumbrstmaynumbrtextfanumbrreceiventriquestionstdtxtratetcapplinumbrovernumbr', 'udunsayearlihorucalreadisay', 'nahdontthinkgoeusflivearoundthough', 'freemsgheydarlnumbrweekwordbackidlikefunstilltbokxxxstdchgsendmoneysymbnumbrrcv', 'evenbrotherlikespeaktreatlikeaidpatent', 'perrequestmellmelloruminnaminungintnurunguvettamsetcallertuncallerpressnumbrcopifriendcallertun', 'winnervalunetworkcustomselectreceiveamoneysymbnumbrprizerewardclaimcallnumbrclaimcodeklnumbrvalidnumbrhour', 'mobilnumbrmonthurentitlupdatlatestcolourmobilcamerafreecallmobilupdatcofreenumbr']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x23pkBk8CtDh",
        "colab_type": "text"
      },
      "source": [
        "Construção do modelo de predição"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wduIzupv_XL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.preprocessing import LabelEncoder # biblioteca para converter valor categórico em discreto\n",
        "\n",
        "# converter rótulos de classes para valores binários, 0 = ham e 1 = spam\n",
        "encoder = LabelEncoder()\n",
        "bin_labels = encoder.fit_transform(classes)\n",
        "\n",
        "# Adicionar em uma lista de tuplas a mensagem e o rótulo da classe, de acordo com suas posições\n",
        "# id 0 com id 0 , 1 com 1...\n",
        "messages = list(zip(processed_lines, bin_labels))\n",
        "\n",
        "#definir um valor de seed para reprodutibilidade\n",
        "seed = 1\n",
        "np.random.seed = stemmed_word\n",
        "\n",
        "# embaralhar as mensagem para que a ordem não interfira no treinamento\n",
        "np.random.shuffle(messages)\n",
        "\n",
        "# inicializar uma lista geral para os vetores de features e os respectivos rótulos\n",
        "featuresets = []\n",
        "\n",
        "for text, label in messages:\n",
        " # Criar lista de palavras tokenizadas para cada linha\n",
        "  word_tokens = word_tokenize(text)\n",
        " # Inicializar um set (conjunto) para as features\n",
        "  features = {}\n",
        " # Para cada palavra existente na lista de tokens modelo\n",
        "for word in word_features:\n",
        " # Se a palavra existir no texto, atribui True,\n",
        " # se não, False\n",
        "  features[word] = (word in word_tokens)\n",
        " # Adiciona na lista geral como tupla de (vetor de\n",
        " #features e rótulos binarizados)\n",
        "  featuresets.append((features, label))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SszGTyqLRJrU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "messages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzSYrcxaHWYw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9eb1e3c3-0659-436c-c06b-57e70301d555"
      },
      "source": [
        "# lib para dividir o dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# dividir os dados em dois conjuntus; 75% para treinamento e 25% para teste\n",
        "training, testing = train_test_split(\n",
        "    featuresets, #dataset com as características e rótulos\n",
        "    train_size = None,\n",
        "    test_size = 0.25, \n",
        "    random_state =seed\n",
        ")\n",
        "# Imprimir o número de instâncias do conjunto de treinamento\n",
        "print(len(training))\n",
        "\n",
        "# Imprimir o número de instâncias do conjunto de teste\n",
        "print(len(testing))\n",
        "\n",
        "test_features, test_labels = zip(*testing)\n",
        "train_features, train_labels = zip(*training)\n",
        "\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1125\n",
            "375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvCxgdhWKMqa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "32c79a2a-f387-4400-b57c-2fa2d56e729c"
      },
      "source": [
        "# Biblioteca do classificador multinomial Naive Bayes\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "# lib para vetorizar dicionarios\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "# instanciar\n",
        "vect = DictVectorizer(sparse=False)\n",
        "\n",
        "# Vetorizar os dados de treinamento e teste\n",
        "train_features = vect.fit_transform(train_features)\n",
        "test_features = vect.fit_transform(test_features)\n",
        "\n",
        "# Instanciar o modelo Naive Bayes\n",
        "model = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
        "\n",
        "# Treinar o modelo com os dados de treinamento\n",
        "model.fit(train_features, train_labels)\n",
        "\n",
        "# Testar o modelo com os dados de teste\n",
        "predict = model.predict(test_features)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-ca3bc8874f20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Vetorizar os dados de treinamento e teste\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtest_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/_dict_vectorizer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mFeature\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0malways\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/_dict_vectorizer.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, fitting)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;31m# same time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%s%s%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseparator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'items'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewPIh9zHLaCw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "61ac5b3a-6fd6-4663-9333-648f10ee3df5"
      },
      "source": [
        "# lib para gerar matriz de confusão\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# gera matriz de confusão a partir dos resultados preditos\n",
        "confus_matrix = confusion_matrix(test_labels, predict)\n",
        "\n",
        "# cria um DF adicionando nomes de linhas e colunas\n",
        "pd.DataFrame(\n",
        "    confus_matrix,\n",
        "    index = [['Real', 'Real'], ['spam', 'ham']],\n",
        "    columns = [['Predito', 'Predito'], ['spam', 'ham']]\n",
        ")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   1653\u001b[0m                 blocks = [\n\u001b[0;32m-> 1654\u001b[0;31m                     \u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1655\u001b[0m                 ]\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype)\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3053\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim)\u001b[0m\n\u001b[1;32m    124\u001b[0m             raise ValueError(\n\u001b[0;32m--> 125\u001b[0;31m                 \u001b[0;34mf\"Wrong number of items passed {len(self.values)}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m                 \u001b[0;34mf\"placement implies {len(self.mgr_locs)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Wrong number of items passed 1, placement implies 2",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-7d79abac3fcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mconfus_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Real'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Real'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'spam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ham'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predito'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Predito'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'spam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ham'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    462\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;31m# For data is list-like, or Iterable (will consume into list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_ndarray\u001b[0;34m(values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mblock_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   1662\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"values\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m         \u001b[0mtot_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1664\u001b[0;31m         \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   1692\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mblock_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1694\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of passed values is {passed}, indices imply {implied}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (1, 1), indices imply (2, 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5flt-w-MliE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "24fc6ee8-27db-499f-8f79-b09133e0763f"
      },
      "source": [
        "print(test_features)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3P8404PPH3Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "4f3a853f-2c84-4ad7-ced0-66acd668cd22"
      },
      "source": [
        "train_features"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lnOZFPWPKlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}